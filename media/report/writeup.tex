\documentclass[10pt,twocolumn,letterpaper]{article}
%\usepackage[latin1]{inputenc}

%\usepackage{url}
%\usepackage{booktabs}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{nonfloat}
\usepackage{url}
\usepackage[colorlinks=true, linkcolor=green, pagebackref]{hyperref}
\usepackage{textcomp} % for textonehalf
%\usepackage{subfig}
%\usepackage{subref}

\usepackage{wrapfig}
\graphicspath{{imgs/}}

\usepackage{xspace}
\renewcommand*{\eg}{e.g.\@\xspace}
\renewcommand*{\ie}{i.e.\@\xspace}
\newcommand*{\ea}{et al.\@\xspace}
\renewcommand*{\vs}{vs.\@\xspace}
%\renewcommand{\arraystretch}{1.5}

%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% General notation
\newcommand{\prob}{Pr}
\newcommand{\degree}{^{\circ}}
\newcommand{\feat}{\varphi}

% Image notation
\newcommand{\rgbdimage}{\mathcal{D}}
\newcommand{\intrinsics}{K}
\newcommand{\pixelidx}{\mathbf{s}}
\newcommand{\edgeimidx}{\mathbf{e}}

% Voxel notation
\newcommand{\voxelgrid}{\mathcal{V}}
\newcommand{\voxel}{v}
\newcommand{\voxidx}{i}
\newcommand{\voxelidxs}{m, n, l}

% Point cloud notation
\newcommand{\project}{\mathbf{p}}
\newcommand{\pcloud}{\mathcal{P}}
\newcommand{\point}{\mathbf{p}}
\newcommand{\normal}{\mathbf{n}}
\newcommand{\updir}{\mathbf{u}}

% Transformations
\newcommand{\trans}{T}
\newcommand{\extrinsics}{H}
\newcommand{\voxelgridtoworld}{\trans_{\voxelgrid \rightarrow w}}


\definecolor{red}{rgb}{0.95,0.4,0.4}
\definecolor{blue}{rgb}{0.4,0.4,0.95}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{grey}{rgb}{0.6,0.6,0.6}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\note}[1]{\textcolor{blue}{NOTE: #1}}
\newcommand{\status}[1]{\textcolor{blue}{Status: #1}}
\newcommand{\add}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\remove}[1]{\textcolor{grey}{#1}}



%[citecounter=true, style=ieee]
%\usepackage{biblatex}
%\addbibresource{bibtex/strings.bib}
%\addbibresource{bibtex/main.bib}
%\addbibresource{bibtex/crossrefs.bib}
%addbibresource{\jobname.bib}


\title{Structured Prediction of Unobserved Voxels From a Single Depth Image}

%\author{Michael Firman, Gabriel Brostow, Simon Julier \ea}

\begin{document}


\maketitle

\begin{abstract}
	%Building a representation of the geometry of a scene is an essential task for many applications including robotic navigation, scene re-lighting and object manipulation. 
	%Most existing works to recover the scene geometry rely on combining multiple views of the scene captured from many different directions or use of \emph{a priori} information about the expected semantic make-up of the scene.

  Building a complete 3D model of a scene is underconstrained given a single camera view.
  To gain a full volumetric model, we typically need either multiple views, or a single view together with the strong assumption that we have a library of unambiguous training instances that we can use to fit the shape of each individual object in the scene.

  We hypothesize that objects of dissimilar semantic classes often share similar shape components, enabling a limited dataset of objects to model the shape of a wide range of objects, and hence estimate the hidden geometry of various scenes.
  Exploring this hypothesis, we have implemented a system which can complete the unobserved geometry of a scene using a library of simple volumetric elements learned from training data.
  Using a novel feature representation, we train a model to map from observed depth values to an estimate of surface shape in the region surrounding a 3D location.
  Through the prototype, we validate our approach qualitatively and quantitatively on a range of indoor scenes.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%\footnote{Potential to add to intro: Psychology: Are humans good at this? Not much evaluation in other work.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% What is the problem?
We broadly categorize space in our world as being `occupied' and opaque, or `vacant' and transparent.
Depth cameras such as the Microsoft Kinect are able to give an estimate of which regions of a scene are composed of free, vacant space.
However, each pixel in a depth image only makes an estimate of occupancy in front of the first solid surface encountered along that camera ray (figure \ref{fig:intro}).
The `occlusion phenomenon' prevents any information from being measured about the occupancy of space beyond that first surface.

% Why is it interesting and important?
There are many applications, however, which critically require a complete representation of the world geometry.
When a robot sets out to grasp an unknown object in an unknown scene, a 3D model is required to get there and prevent collision with the object or nearby clutter.
Separately, in photo-editing, the full geometry could enable realistic shadows from a new light source to be automatically added to an image after it has been captured.

% Why is it hard? (E.g., why do naive approaches fail?)
Much research in computer vision has reconstructed the full 3D world from images of a scene captured from multiple viewpoints, thus coping with the effects of occlusion  (\eg \cite{izadi-uist-2011}).
Instead, we focus on the task of classifying each voxel in a 3D scene as being either `occupied' or `vacant' given just a single depth image from one viewpoint.
%% THERE WILL ALWAYS BE HOLES, EVEN WITH KINFU %%

\newcommand{\introsubwidth}{0.48\columnwidth}
\begin{figure}[!t]
    \centering 
    \subfigure[Voxel world model]{%
        \includegraphics[width=\introsubwidth, clip=true, trim=110 105 205 30]{fig_1}}
        \hfill
    \subfigure[Depth rendering of world]{%
        \includegraphics[width=\introsubwidth, clip=true, trim=110 105 205 30]{fig_1b}}
    \caption{
    We assume that our world can be represented as voxels, each of which is either occupied or empty (vacant). 
    An overhead view of a coarse 2D representation of this is shown in (a).
    When observed by a depth camera, only the first voxel along each ray is seen.
    This leaves a region of unknown occupancy extending beyond the depth surface (b).
    The aim of our algorithm is to predict the state of the voxels in this unknown region.}%
    \label{fig:intro}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Our approach and contributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given a single depth image, our system predicts whether each voxel in the scene is occupied by a solid impermeable object, or free and empty.
In effect, we strive to predict the voxelized output of KinectFusion \cite{izadi-uist-2011}, but having test-time input of only a single view of the scene instead of multiple views.

We achieve this by learning a mapping from local and novel semi-regional features to a structured prediction of geometry in the region of a query point, using a general-purpose collection of training objects and scenes.
We take inspiration from recent work which has segmented objects from images using silhouettes learned from different object classes \cite{kim-eccv-2012}.
Their work showed that shape can transcend class categories, enabling shape predictions to be made regardless of the accuracy of semantic classifiers.
Because we care about shape independently of semantic understanding, we are free to use training objects which differ from the objects being modeled in the scene.

The key contributions that underpin our novel depth image to voxel geometry framework are:
\begin{itemize}
\item \note{Tom says that being able to ``solve the problem as stated'' should be on this list. Opinions?}
\item \emph{Voxlets}, a representation of multi-voxel geometry in the region of a point in a scene. 
We use a Random Forest to learn a mapping from a point in a depth image to a structured prediction of geometry in the region near the point.
\item A novel feature representation for a point in a depth image, which captures both local and region-level shape while respecting occlusions.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Most prior work in the area of completing missing data can be categorized according to its application domain (\eg meshes \cite{schnabel-eurographics-2009, ju-cst-2009}, 2D images \cite{gupta-cvpr-2011} or depth images \cite{shen-tog-2012}).


%\subsection{Taxonomy of related works}
Previous works on completing unknown regions of visual data can be categorized against several different criteria.
Some of these are given here, and we italicize the categories into which our approach falls.

Works can be classed according to their application domain, such as operating on meshes \cite{harary-tog-2013, schnabel-eurographics-2009}, 2D images \cite{gupta-cvpr-2011} or in \emph{voxel space} \cite{kim-iccv-2013}.
Some works make use of highly specialist hardware to capture images \eg \cite{velten-nature-2012}, while we use \emph{hardware available to consumers}.
Some approaches only aim to get a result that looks plausible to a human, while we strive for \emph{accuracy in comparison to the ground truth}.
In comparison to works which use heuristics, we make use of \emph{training data}.
Such `supervised' approaches can be further classed according to whether the data used come from within the same image (\eg symmetry \cite{kroemer-humanoids-2012}) or from a \emph{database of training data}.

We now outline some of these previous approaches in more detail, and compare them to our approach.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Fitting 3D models}
If prior knowledge is available about the objects present in the scene, in the form of 3D models, then an instance-level model can be fitted to a 3D scene.
This gives a good recovery of missing geometry of that object \cite{hinterstoisser-accv-2012, drost-3dimpvt-2012}.
Some works focus on the broader problem where an exact match is not present in the training set.
Shen \ea \cite{shen-tog-2012} complete the missing regions of a single object using an assembly of parts from several different models in a CAD database.
Cocias \ea \cite{cocias-cgvcv-2013} directly deform a mesh to fit to a point cloud, while Prisacariu and Reid \cite{prisacariu-iccv-2011} fit a class-level manifold model to the image data.

All these methods, however, rely on the availability of some form of specific prior model.
Collecting training data for this is laborious and costly, and currently there do not exist enough 3D models suitable for accurately completing most real-world scenes.
Furthermore, this completion method relies on an accurate detector to find and classify each object in the scene.
In our approach we set out to get as much shape information as possible \emph{without semantics}, thus remaining free of its associated machinery and limitations.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Symmetry}
Where one can accurately detect symmetry, it can be leveraged to complete some types of objects (\eg \cite{law-cviu-2010, thrun-iccv-2005, kroemer-humanoids-2012}). 
However, using symmetry for object completion is brittle.
An incorrectly guessed symmetrical transform can lead to a catastrophic mis-estimate of geometry, and if symmetry cannot be detected at all then no prediction can be made.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Voxel space reasoning}
The two algorithms which bear the most similarity to ours both make predictions of full scene geometry from a single depth image.

Kim \ea \cite{kim-iccv-2013} use a CRF model over a voxel representation of a scene to simultaneously predict occupancy, visibility and semantic labeling of voxels from an RGBD image. 
For training, they use manually labeled top-down views of the scene.
Their final labellings are found from graph cuts over the CRF.
They primarily model the probability of a voxel being occupied as a Gaussian centered on the first observed voxel along a camera ray.
However, high-order-terms in the CRF are used to enforce planar structures and for `objects' to remain contiguous.

Like \cite{kim-iccv-2013},  Zheng \ea \cite{zheng-cvpr-2013} go from a single depth image to a voxel representation of a scene.
Their core algorithm consists of two parts.
Firstly they complete missing voxels by extruding visible points in the detected Manhattan World directions of the scene.
(This bears resemblance to a similar method used for completions by \cite{kroemer-humanoids-2012}.)
Secondly, they use physics-based reasoning to fill in missing data to ensure connectedness and stability.
The physics-based reasoning is clearly useful in complex scenes where, for example, a table leg may be occluded.
However the voxel completion by extrusion is limited by the Manhattan World assumption, and the extent of visible voxels in the scene.

\note{In contrast to these methods, by making structured predictions we are able to complete objects where the points making up the objects are hard to see.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Surface completion}
Silberman \ea \cite{silberman-eccv-2014} tackle the completion of an incomplete multi-view reconstruction as a surface completion problem.
By detecting planes they can complete their 3D contours in 2D space using a novel CRF method.
This method, however, relies both on a piecewise-planar scene, and on beginning with an almost-complete scan as input.

Davis \ea \cite{davis-3dpvt-2002} complete surfaces by operating directly on the \emph{signed distance field}, the zero level-set of which defines the surface location. They diffuse the signed distance field across holes in the mesh to fill in the gaps.
On the other hand, \cite{harary-tog-2013} take a data-driven approach.
They find matches in the mesh to the missing region, and blend the result in.

All of these completion methods are only suitable where the set of missing data is small relative to the size of the observed data.
In our case, we are completing an unknown surface which is typically at least as large as the observed surface.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Image completion and super-resolution}
Image completion works such as \cite{hays-siggraph-2007, criminisi-cvpr-2003}
typically use region-based data-driven approaches as it is very difficult to form true generative models over image appearance.
For example, \cite{hays-siggraph-2007} look up possible completion regions in a large database of similar images, while \cite{criminisi-cvpr-2003} in-paint by selecting and combining multiple plausible patches from other regions of the input image.
This differs from our task as image completion typically aims for a visually plausible output, irrespective of the accuracy compared to ground truth.
Similar to image \emph{completion} is image \emph{super-resolution}, \eg \cite{macaodha-eccv-2012}. 
This, like our problem, is ill-posed and relies on the repeatability and predictability of the world in order to improve the local geometry of an image.
Our problem differs from that of super-resolution because we make predictions about the state of the world outside of the region directly observable by the camera.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Using 3D primitives for recognition}
`Geons' are proposed by \cite{bieberman-rbc-1987} as a set of 3D primitives such as cylinders and cuboids used by humans in their recognition of object shapes.
While in theory geons could be used by computers  as features to describe natural objects, in practice this was found to be challenging due to their ``idealized nature'', requirement for part segmentation, labeling errors and the coarseness of features used to extract geons in the first place \cite{dickinson-iavc-1997}.
%Besl - hand-defined

However, fitting bounding boxes has recently become a popular method to explain the arrangement of objects in a scene.
Recent work has successfully incorporated high-level information such as gravity and stability
 \cite{shao-siggraphasia-2014, jia-cvpr-2013}, and made use of training data to accurately detect bounding box locations \cite{hedau-cvpr-2012}.
Gupta \ea \cite{gupta-cvpr-2011} estimate voxel occupancy from a 2D image, which is regularized using cuboid bounding box hypotheses.
The obvious problem with bounding box style methods is that they can only give coarse shape information, which is not suitable for many applications of geometry completion.

In our work we make use of 3D primitives.
However, unlike geons which are fixed in shape, we learn a distribution of shapes from training data.
We are also able to make more fine-grained predictions than bounding boxes.



\begin{figure*}[!htb]
    %\centering 
    \subfigure[Query point]{%
        \includegraphics[width=0.5\columnwidth, clip=true, trim=110 170 205 30, page=1]{overview_image}\label{subfig:voxregion}}
        \hfill
    \subfigure[Forest prediction]{%
        \includegraphics[width=0.45\columnwidth, clip=true, trim=80 170 265 30, page=2]{overview_image}}
        \hfill
    \subfigure[Insertion into voxel grid]{%
        \includegraphics[width=0.5\columnwidth, clip=true, trim=110 170 205 30, page=3]{overview_image}}
        \hfill
      \subfigure[Multiple predictions]{%
        \includegraphics[width=0.5\columnwidth, clip=true, trim=110 170 205 30, page=4]{overview_image}}
    \caption{\textbf{Overview of our algorithm, shown diagrammatically in a 2D representation.} 
    (a) At test time, we define a cuboid region of voxels $\mathcal{R}$ around a query point $\pixelidx$.
    This cuboid is aligned with the normal at $\pixelidx$.
    (b) A feature representation $\mathbf{x}(\mathbf{s})$ is put through our pre-trained Random Forest.
    The forest makes a prediction for the values of each of the voxels in $\mathcal{R}$.
    (c) This prediction is transformed into the scene and used to update the values of the voxels.
    (d) The aggregation of multiple such predictions forms our final prediction of the TSDF, which can be converted to voxel occupancy or a surface representation.
    }%
    \label{fig:overview}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Structured learning for vision}
In recent years \emph{structured learning} has become a popular method for some computer vision tasks.
As with standard supervised learning, structured learning finds a mapping from feature space to a label space.
In contrast to the traditional classification paradigm, however, a structured label space is \emph{multidimensional}.
Feature space is typically limited to a local descriptor of the image, while the label space may be surface normals \cite{fouhey-iccv-2013}, human poses \cite{bourdev-iccv-2009} or semantic labels.
This family of works provides inspiration for our approach.

%A question that arises is: How to form the set of primitives?
There are many different ways of finding the mapping from feature to structured label space.
For example, \cite{bourdev-iccv-2009} cluster human poses, while \cite{fouhey-iccv-2013} use an SVM-like formulation to find primitives which are both discriminative in feature space and informative in label space.

In our work we make use of Random Forests \cite{breiman-ml-2001}.
Originally proposed for regression and then single-label classification problems, they have since been adapted to be able to make structured predictions for tasks such as semantic labelling \cite{kontschieder-iccv-2011} and edge detection \cite{dollar-iccv-2013}.
Structured prediction can make faster and more regularized predictions than a single dimensional predictor.
In our case, however, we benefit from a structured prediction as we are able to make predictions beyond the location of the input data point.

%One example of this is work on edge detection, where an input image patch is mapped to a prediction of the edges in the region of the patch 
% \begin{quote}
% Voxel occupancy is one approach for reconstructing the 3-dimensional shape of an object from multiple views. In voxel occupancy, the task is to produce a binary labeling of a set of voxels, that determines which voxels are filled and which are empty.
% \cite{snow-cvpr-2000}
% \end{quote}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Approach formulation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



We model the geometry of a scene as a regular grid of voxels $\voxelgrid = \{\voxel_\voxidx\}$.
Following works such as \cite{izadi-uist-2011, prisacariu-iccv-2011}, each $\voxel_\voxidx \in [-d_{\max}, d_{\max}]$ represents the \emph{truncated signed distance function} (TSDF) of the surface of the scene.
Each $|\voxel_\voxidx|$ gives the distance from $\voxel_\voxidx$ to the nearest surface, truncated to a maximum value of $d_{\max}$, which is a parameter. 
$\voxel_\voxidx$ is negative if voxel $\voxidx$ is inside solid opaque matter, and positive if it is in free space. 
The zero level-set of $\voxelgrid$ therefore represents the location of the surface.

\newcommand{\voxregion}{\mathcal{R}}

Our system maps a point $\pixelidx$ on an input depth image $\rgbdimage$ to a prediction of the TSDF in a set of voxels in the neighborhood of $\project(\pixelidx)$, where $\project(\pixelidx)$ is the 3D location of $\pixelidx$ in world space.
The aggregation of multiple such predictions gives our final TSDF prediction for the scene.

%\paragraph{Voxel grid alignment}
%We make our predictions in a voxel grid aligned with world coordinates.
%In practice this means that the z direction of the voxel grid is aligned with the up direction of the world space.
%We do not make any Manhattan World assumptions so are free to position the voxel grid at any orientation about the z-axis.


\paragraph{Support regions}
The \emph{support region} $\voxregion_\pixelidx \subset \voxelgrid$ is a set of voxels in the neighborhood of $\pixelidx$ for which our model can make a prediction of geometry.
Each $\voxregion$ is a fixed-size cuboid of voxels aligned with the normal direction at $\pixelidx$ (figure \ref{subfig:voxregion}).
In 2D, the location of $\pixelidx$ and the direction of its normal can unambiguously define the location and orientation of $\voxregion$.
However, in 3D there is a degree of freedom unconstrained as the rotation of the cuboid about the axis of the normal is unspecified.
We resolve this by aligning the cuboid such that its z direction is coincident with the world z-axis.
The top and bottom face of each cuboid region $\voxregion$ is therefore parallel with the world's ground plane.
The normal at $\pixelidx$ is used to contrain the degree of freedom around the z-axis.
\todo{Figure for this!}
%The cuboid is then rotated about the z-axis so that its y-axis is perpendicular to the normal at $\project(\pixelidx)$.

% \remove{
% Each $\voxregion$ is aligned with the coordinate system
% \begin{equation}
%   \Lambda = 
%   \left( \begin{array}{ccc}
%   \updir \times \normal \,\, / \,\,  \| \updir \times \normal \| \\
%   \updir \times (\updir \times \normal) \,\,  / \,\,  \| \updir \times (\updir \times \normal) \|  \\
%   \updir 
%   \end{array}\right),
%   % \qquad
%   %(\mathbf{v})_{u} := \mathbf{v} / \| \mathbf{v} \|,
% \end{equation}
% where $\normal$ is the normal at $\point$ and $\updir$ is the `up' direction of the scene (\ie pointing directly away from the direction of gravity).

%We examine each $\feat(\pixelidx)$ from $\rgbdimage$ in turn.
\paragraph{Making a single prediction}
At test time, we map a pixel $\pixelidx$ to a feature representation $\feat(\pixelidx)$, as described in section \ref{sec:features}.
Using a structured Random Forest, we can make a prediction of the geometry inside of $\voxregion_\pixelidx$.
We call this prediction of geometry $\mathcal{S}$ a \emph{voxlet}.
The voxlet, which comes out of the forest in canonical alignment, is then transformed from its local coordinate system into world space to fill the voxels in $\voxregion$.

The accumulation of multiple such predictions forms our final prediction of our TSDF.
This can then be converted to a prediction of surface geometry, a process which is described in section \ref{sec:combining}.

%This transformation is the rotation $\Lambda$ followed by the translation $\mathbf{p}(\mathbf{\pixelidx}) - C(\mathcal{S})$, where $\mathbf{p}(\mathbf{\pixelidx})$ is the projection of $\pixelidx$ into world space, and $C(\mathcal{S})$ is the center of the voxlet. 
%\note{This notation and explanation is a bit of a mess --- suggestions welcome! Also need to reference the figure in the text. Figure will ultimately be split into (a), (b) etc to make this easier.}


\paragraph{Training}
At training time, we similarly define a region $\voxregion$ around each point $\pixelidx$, again of a fixed size. 
In this case, the ground truth values of $\voxelgrid$ and hence $\voxregion$ are known. 
We use these ground truth TSDF values, together with $\mathbf{x}(\pixelidx)$, to train a Random Forest, as explained in  section \ref{sec:forest_train}.


\newcommand{\preprocesssubwidth}{0.41\columnwidth}
\begin{figure*}[tb]
    \centering 
    \subfigure[RGB image]{%
        \includegraphics[width=\preprocesssubwidth]{preprocess_a}}
        \hfill
    \subfigure[Raw depth image]{%
        \includegraphics[width=\preprocesssubwidth]{preprocess_b}}
        \hfill
    \subfigure[Smoothed depth]{%
        \includegraphics[width=\preprocesssubwidth]{preprocess_c}}
        \hfill
    \subfigure[Structured edges]{%
        \includegraphics[width=\preprocesssubwidth]{preprocess_d}\label{subfig:structedge}}
        \hfill
    \subfigure[Binary edge map]{%
        \includegraphics[width=\preprocesssubwidth]{preprocess_e}\label{subfig:binaryedge}}
    \caption{
    The depth image preprocessing pipeline.
    The noisy depth image (b) is smoothed, and missing data is filled (c). 
    This makes use of the RGB image in the cross-bilateral filtering, where we use the implementation provided by \cite{silberman-eccv-2012}.
    We then use the structured edge detection model from \cite{dollar-iccv-2013} to compute a real-valued edge map (d) for the image, which is finally binarized (e) using the Canny edge hysteresis method \cite{canny-pami-1986}.
    }%
    \label{fig:preprocessing}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Feature representation}
\label{sec:features}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Our feature representation maps a pixel location $\pixelidx = (u, v)$ to a feature $\feat(\pixelidx)$, which represents the geometry around the point.
Our feature representation $\feat(\pixelidx)$ at location $\pixelidx$ in the depth map describes aspects of its neighborhood.
We use local and regional features extracted from the depth image.
The Random Forest--based model weights feature space to make structured predictions of the truncated signed distance function in the surrounding region.
%This is used in our classifier to map from a single point to a prediction of voxel occupancy in the surrounding region.

Our feature $\feat(\pixelidx) = [\feat_c(\pixelidx), \feat_s(\pixelidx)]$ is composed of two parts.
The \emph{spider} feature $\feat_s(\pixelidx)$ captures the size and shape of the region in which $\pixelidx$ resides, together with the position of $\pixelidx$ in that region.
The \emph{cobweb} feature $\feat_c(\pixelidx)$ captures the shape of the depth surface in the immediate neighborhood of $\pixelidx$.
%\footnote{The names of our features are taken from the shapes they produce in image space (figure \ref{fig:features}).}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cobweb feature }
\note{Motivation}
%\note{Name up for debate!}
\begin{wrapfigure}{r}{0.35\columnwidth}
    \includegraphics[width=0.35\columnwidth, clip=true, trim=150 190 340 100, page=1]{spider_cobweb}
  \caption{A gull}
\end{wrapfigure}

The cobweb feature $\feat_c(\pixelidx)$ is a simple pairwise feature, inspired by recent work such as \cite{shotton-cvpr-2011, tola-pami-2010}, capturing the surface shape in the immediate neighborhood of pixel $\pixelidx$.
The feature computes the difference between the depth from the camera at $\pixelidx$ and the depth at a predetermined offset. We define
\begin{align}
\Phi(\pixelidx, \psi, t) &= \rgbdimage(\pixelidx) - \rgbdimage(\pixelidx + \Delta), \, \text{where} \\
\Delta &= M(t) (\sin(\psi), \cos(\psi)).
\end{align}
Here $M(t) = \frac{t\cdotp f}{\rgbdimage(\pixelidx)}$ uses the focal length $f$ to map a distance in world space to a pixel offset. 
The final feature is then composed of $\Phi(\pixelidx, \psi, t)$ evaluated over a range of values of $\psi$ and $t$.
For our experiments, we compute the cobweb feature for all combinations of $\psi = [0\degree, 45\degree, \ldots, 315\degree]$ and $t = [0.02m, 0.04m, 0.06m, 0.08m]$.
The final cobweb feature is therefore 24-dimensional.
The pattern of the extracted $\pixelidx + \Delta$ values can be seen in figure \ref{subfig:cobweb}.


  %\end{center}
  %\begin{center}
%In our experiments the final feature is then composed of 
% as
%\begin{align}
%\feat_c(\pixelidx) &= [\Phi(\pixelidx, \psi, t): \psi = [0\degree, 45\degree, \ldots, 315\degree], t = [0.%02m, 0.04m, 0.06m, 0.08m].
%\end{align}

%a &= \left\lfloor u + M(t)  \sin(\psi) \right\rceil \\
%b &= \left\lfloor v + M(t)  \cos(\psi) \right\rceil

% over a range of values of  
%Note also: ``If an offset pixel lies on the background or outside the bounds of the image, the depth probe dI (x0) is given a large positive constant value''.


\newcommand{\subwidth}{0.32\columnwidth}
\begin{figure}[tb]
    \centering 
    \subfigure[Query point]{%
        \includegraphics[width=\subwidth, clip=true, trim=130 125 300 30, page=1]{spider_cobweb}}
        \hfill
    \subfigure[Cobweb feature]{%
        \includegraphics[width=\subwidth, clip=true, trim=130 125 300 30, page=2]{spider_cobweb}
        \label{subfig:cobweb}}
        \hfill
    \subfigure[Spider feature]{%
        \includegraphics[width=\subwidth, clip=true, trim=130 125 300 30, page=3]{spider_cobweb}}
        \\
    \subfigure[$s_0$]{%
        \includegraphics[width=\subwidth, clip=true, trim=130 105 300 30, page=4]{spider_cobweb}\label{subfig:c0}}
        \hfill
    \subfigure[$s_1$ (Top view of scene)]{%
        \includegraphics[width=\subwidth, clip=true, trim=130 70 300 30, page=5]{spider_cobweb}\label{subfig:c1}}
        \hfill
    \subfigure[$s_2$ (Top view of scene)]{%
        \includegraphics[width=\subwidth, clip=true, trim=130 70 300 30, page=6]{spider_cobweb}\label{subfig:c2}}
        \hfill
    \caption{
    \todo{Clean up pictures to have notation consistent with text and in correct places on images}
    (a) We compute a feature vector for a single point $\pixelidx = (u, v)$.
    (b) The cobweb feature computes the difference in depth between $\pixelidx$ and the points that surround it.
    (c) The spider features $\mathbf{x_s} = [s_0, s_2, s_2$ measure the distance between $\point$ and the edge points $\edgeimidx_{1, \cdots, 7}$.
    (d) $s_0$ measures the distance between $\pixelidx$ and $\edgeimidx$ in the image space.
    (e) $s_1$ measures the distance between $\pixelidx$ and $\edgeimidx$ along the surface of $\rgbdimage$
    (f) $s_2$ gives an estimate of the depth of the object at the location of $\pixelidx$.
    }%
    \label{fig:features}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The spider feature}
%\note{Name up for debate! Another candidate is `compass'}

Local features, such as our cobweb feature, have proved discriminatory where the output labeling is on a per-pixel basis, such as labeling each pixel as a part of a human body \cite{shotton-cvpr-2011}.
However, when it comes to predicting the geometry of the scene we cannot observe, the local shape of the depth image around a point does not provide enough information. 
An example of this can be seen in figure \ref{fig:patch_problems}.
The two regions circled in the image have identical \emph{local} appearance, but the 3D geometry around each region is very different due to the different thicknesses of the two boxes.


\begin{figure}[bt]
  \centering 
  \includegraphics[width=0.9\columnwidth]{features_1}
  \caption{Patch-type features do not give enough information to accurately predict depth. Here, regions on two different objects are marked. Each region has identical local appearance. However, the voxel geometry of the object at the two locations is very different due to the different thicknesses. 
  We use this observation to motivate the spider feature.}
  \label{fig:patch_problems}
\end{figure}
%\todo{Combine all the motivational images for the spider feature into one...}}


Previous works have used the properties of a whole segmented region for classification of individual points \cite{golovinskiy-iccv-2009}.
When it comes to predicting the 3D shape local to a point in a depth image, however, we care as much about the point's location in the region as we do about the overall region properties (figure \ref{fig:spider_motivation}).

We desire a feature which captures the size and shape of the region in which $\pixelidx$ resides, along with the location of $\pixelidx$ in that region.
To achieve this we take inspiration from the features between a point and a binary edge map used by \cite{drost-3dimpvt-2012}.

\paragraph{Computing a binary edge map}
We first compute a binary edge map for the image, where each pixel takes the value 1 at a large change in depth or surface normal direction, and is 0 otherwise.
We first use the method of \cite{dollar-iccv-2013} to compute a real-valued edge map for the image (figure \ref{subfig:structedge}).
Next we apply the Canny filtering algorithm \cite{canny-pami-1986} to convert this to a binary edge map (figure \ref{subfig:binaryedge}).
We mitigate against poor quality depth data around discontinuities by dilating this edge map with a 3x3 disc-shaped structuring element.

\paragraph{Casting lines from $\pixelidx$}
From $\pixelidx$, we then cast a line across the edge map in each direction $\phi$.
We denote the point on the edge map where the line first hits a pixel with value 1 as $\edgeimidx$.
The 2D locations $\pixelidx$ and $\edgeimidx$ reproject to 3D locations $\project_\pixelidx$ and $\project_\edgeimidx$ respectively.
%For each line cast we therefore have two points in 3D space: $\point$ and $\point_e$.





\paragraph{Extracting the spider feature}

\newcommand{\spiderfeat}{s}
\newcommand{\gline}{\mathbf{l}}

The spider feature for a single point with a line cast in direction $\phi$ is $\Phi_s = (\spiderfeat_0, \spiderfeat_1, \spiderfeat_2)$.
Each element of this tuple is a distance between $\pixelidx$ and $\edgeimidx$, computed as follows:
\begin{itemize}

\item $\spiderfeat_0 = \rgbdimage(\pixelidx) \|\pixelidx - \edgeimidx\| / f$. This is the distance between $\pixelidx$ and $\edgeimidx$ in pixel units $\|\pixelidx - \edgeimidx\|$ scaled by $\rgbdimage(\pixelidx) / f$ to convert to a real-world distance (figure \ref{subfig:c0}).

\item $\spiderfeat_1 = \sum_{i=2}^{|\gline|} \| \project_{\gline(i)} - \project_{\gline(i-1)}) \| $, where $\gline$ is an ordered list of the pixel locations on the line between $\pixelidx$ and $\edgeimidx$.
$\spiderfeat_3$ approximates the arc length between $\pixelidx$ and $\edgeimidx$ along the surface of $\rgbdimage$  (figure \ref{subfig:c2}).

\item $\spiderfeat_2 = (\point_\pixelidx - \point_\edgeimidx) \cdot \normal$. 
This is the Euclidean distance between $\point_\pixelidx$ and $\point_\edgeimidx$ along the direction of the normal at $\pixelidx$  (figure \ref{subfig:c1}).

\end{itemize}

We compute the spider features for  $\phi = [0\degree, 45\degree, \ldots, 315\degree]$
The spider feature for a single point $\pixelidx$ is therefore 24-dimensional.

\paragraph{Implementation}
The spider feature is extremely efficient to compute.
We can compute the 24D spider feature for every point in a $640\times480$ image in less than 0.01s using unoptimized C++ code.
We would expect a significant speedup were this to be implemented on a GPU.
%This is due to its cumulative nature: the feature at $\pixelidx$ can be directly computed from $\pixelidx - (0, 1)$.


\paragraph{Occluded spider features}
\todo{Remove or improve and do experiments}

We explicitly handle occlusion in the spider features. Where the spider feature first hits an occluded edge rather than an occluding edge, the true size of the object in that dimension is unknown (see figure \ref{fig:features:occluded_spider}).
However, we do have a \textit{minimum} size in that direction.

We reason that discontinuities in depth images can be assigned a direction: one side of each edge is the occluder, the other is the occludee. 
We can compute the gradient of the depth edges using PCA on the edge pixels in image space.
We ensure that the final gradient at each edge pixel points in the direction of the occluded side.

We can then use the occlusion information in our feature computations.
See for example figure \ref{fig:occluded_region}.


% \begin{figure}
%     \centering 
%     \subfigure[]{%
%         \includegraphics[width=0.45\columnwidth]{occlusion_a}}
%         \hfill
%     \subfigure[]{%
%         \includegraphics[width=0.45\columnwidth]{occlusion_b}} \\
%     \caption{Where edges occur as a result of occlusion boundaries, the direction of the edge is important. The edge bounding region A can be divided into an \emph{occluding} portion (solid green) and an \emph{occluded} portion (dashed red).
%     We can make a reasonable assumption that region A may extend behind object B past the occluded edge.
%     However, region A cannot continue sideways beyond its occluding edge.}
%     \label{fig:occluded_region}
% \end{figure}

% \begin{figure}
%     \centering% 
%     \includegraphics[width=0.7\columnwidth]{occlusion_spider.png}% 
%     \figcaption{The spider feature extends lines from $\point$ to the first occluding edge found along the path. Internal edges (e.g. those found in RGB space) are ignored. Where the line first hits an occluded edge, the true extent is unknown --- this case is denoted as \texttt{?} in this figure.}% 
%     \label{fig:occluded_spider}% 
% \end{figure}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning a mapping from features to voxlets}
\label{sec:forest_train}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\note{TODO}

A Random Forest is a set of decision trees, each of which is trained on a different subset of the training data \cite{breiman-ml-2001}.
At test time, each data point is passed through each tree, and at each node is passed left or right depending on the feature thresholds learned at training time. The final leaf node the data point ends in will vote for a class, based on which training samples ended up at that leaf node.

%Random Forests are well suited to our purpose, as they automatically select the features most suitable for separating the classes (unlike \eg SVMs) and they examine each dimension separately, making them robust to different scaling of each feature. They are also inherently non-linear, perform quick training and inference and are easily parallelizable.

\subsection{Training the forest}

Often the modal class vote from all the trees is taken as a hard prediction of class membership. Instead we follow \cite{} in using... medioid? Modal?

% \cite{bostrom-mla-2007} in using the fraction of votes for each class to approximate a probability of class membership. $p_{ij}$ can be interpreted as the parameter of a categorical distribution.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Combining and regularizing predictions}
\label{sec:combining}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This can be done fairly efficiently as the voxlets are all at a constant orientation in world space.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\note{Train and test split}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quantitative evaluation}

\paragraph{Evaluation criteria}
The aim of our algorithm is to accurately classify free space around objects and in scenes.
Therefore, we report the per-voxel ROC curve for each object.
This is computed by sweeping a parameter over the range of values in the TSDF, and comparing the classification of voxels as being `inside' or `outside' the object with the ground truth occupancy.

%, comparing the prediction $\Pr(o)$ with the ground truth occupancy as provided by KinFu.
The shape of the ROC curve is strongly affected by the region over which the evaluation is performed.
If all the voxels between the camera and the depth image are included in the evaluation, the false positive rate becomes very low, as the voxels in the free space between the camera and scene are `easy wins' for the algorithm. 
%We therefore evaluate over all the voxels 


\subsubsection{Baselines and algorithm variants}

\noindent \textbf{(a) Bounding box} We fit a minimum-area bounding box to the 3D points belonging to the object, as defined by the ground truth object segmentation mask provided by \cite{singh-icra-2014}.
We additionally are careful to remove `flying pixels', as they can have a large adverse effect on bounding box predictions.
Finally, the prediction of voxel occupancy is simply that all voxels inside the bounding box are predicted to be occupied, while those outside are predicted to be empty.

\noindent \textbf{(b) Structured (cobweb only)} Our full structured forest, but only using the cobweb features

\noindent \textbf{(c) Structured (spider only)} Our full structured forest, but only using the spider features

\noindent \textbf{(d) Structured (Cobweb + spider)}  Our full structured forest using both cobweb and spider features.

\noindent \textbf{(e) K-means forest} We use a standard classification Random Forest, the leaf nodes of which vote for items in a dictionary of 200 voxlets formed by running K-means on the training set of voxlets.

\noindent \textbf{(f) Zheng \ea \cite{zheng-cvpr-2013}}



We do not compare to Kim \ea \cite{kim-iccv-2013} due to their dependence upon semantic labellings in the training stage of their algorithm.


\subsection{Turntable dataset}

For our experiments on objects in isolation, we make use of the `Bigbird' dataset \cite{singh-icra-2014}. 
This dataset comes of 125 household objects, captured on a turntable from multiple viewing angles.
The camera poses are well calibrated, and a complete object mesh is provided for each object.
This mesh can be converted to provide us with a ground truth voxel occupancy.

In our train/test split of this dataset, we note that there are many `near duplicate' items in the dataset.
For example, two different varieties of the same product may be included.
To ensure a fair split we group such items together to ensure that each group ends up as a whole in the training or the test section.

% following computed from the matlab script eval_roc, after loading 
% /Users/Michael/projects/shape_sharing/data/voxlets/roc_curve_data.mat
% which was copied from troll,  after being computed by 10_ssds.py
\begin{figure}[bt]
  \centering 
  \includegraphics[width=0.9\columnwidth]{roc_curve.png}
  \caption{ROC curve comparing results from the turntable dataset. 
  \note{See section 7.1.1 for the explanation of the types. OMA is the full forest. Ground truth is fixed, while the level set of the TSDF predictions is swept. Bounding box is currently converted to a TSDF which might not make sense.}
  \todo{Also implement a bounding box fitting to the visible data, not just to the ground truth. Spider only is not so good. This is disappointing. Perhaps should revisit this feature if there is time. It may be the case that bad quality data at the edges are causing the feature to perform badly. Perhaps this shouldn't be a graph and instead should be a table of precision recall values.}
  }
  \label{fig:bigbird_roc}
\end{figure}
  %The dot on each curve shows the value of the TPR and the FPR at the level set of 0 in the predicted result.


% \begin{figure}
%     \centering% 
%     \includegraphics[width=1.0\columnwidth]{guo.png}% 
%     \figcaption{Representation of objects in the NYU dataset, provided from \cite{guo-iccv-2013}.
%     Not a perfect representation but perhaps reasonable to some extents.}% 
%     \label{fig:guo_labels}% 
% \end{figure}


\newcommand{\voxletsubwidth}{0.41\columnwidth}
\begin{figure}
     \includegraphics[width=0.32\columnwidth, clip=true, trim=150 150 150 150]{voxlets/1_marching_cubes.jpg}
     \includegraphics[width=0.32\columnwidth, clip=true, trim=150 150 150 150]{voxlets/5_marching_cubes.jpg}
     \includegraphics[width=0.32\columnwidth, clip=true, trim=150 150 150 150]{voxlets/10_marching_cubes.jpg} \\
     \includegraphics[width=0.32\columnwidth, clip=true, trim=150 150 150 150]{voxlets/19_marching_cubes.jpg}
     \includegraphics[width=0.32\columnwidth, clip=true, trim=150 150 150 150]{voxlets/38_marching_cubes.jpg}
     \includegraphics[width=0.32\columnwidth, clip=true, trim=150 150 150 150]{voxlets/44_marching_cubes.jpg}
     \caption{Some of the voxlets learned from the Bigbird dataset. \todo{Perhaps instead show leaf nodes. Describe in text what is going on.}}
\end{figure}


\newcommand{\resultswidth}{0.41\columnwidth}
\begin{figure}
     \includegraphics[width=0.48\columnwidth]{results/nutrigrain_harvest_blueberry_bliss_NP2_216}
     \includegraphics[width=0.48\columnwidth]{results/pop_secret_butter_NP2_312}\\
     \includegraphics[width=0.48\columnwidth]{results/pringles_bbq_NP2_0}
     \includegraphics[width=0.48\columnwidth]{results/progresso_new_england_clam_chowder_NP2_312}\\
     \includegraphics[width=0.48\columnwidth]{results/quaker_big_chewy_chocolate_chip_NP2_312}
     \includegraphics[width=0.48\columnwidth]{results/red_cup_NP3_0}
     \caption{Slices through the TSDF for some of our results, comparing each one to the GT. 
     Our prediction is on the left in each image.
     Each image is set up like a 2d orthographic projection, showing a slice mid-way through the voxel volume in each of the 3 main directions.
     The scaling is wrong on the colorbars so ignore that for now.
     \todo{Redo these in 3D or something. Show some comparisons to baselines etc.}
     Would be good also to include the RGB/depth input to see what the objects are!}
\end{figure}



% \begin{figure}
%     \centering% 
%     \includegraphics[width=1.0\columnwidth]{synth_scene.png}% 
%     \figcaption{One of the user-created synthetic scenes from \cite{fisher-siggraphasia-2012}.}% 
%     \label{fig:fisher_scene}% 
% \end{figure}

\section{Conclusions and future work}


The primary direction for our future work is to apply the algorithm to larger and more complex scenes.
We would like to incorporate ideas from \cite{zheng-cvpr-2013, shao-siggraphasia-2014} by incorporating physics-based reasoning to help the completion of ambiguous regions.

An interesting potential application of our method is to use the predicted completion as a prior for structure-from-motion.
We plan to implement a system which updates our prediction of occupancy with observed data as it arrives from the capture device.
Using our prediction as a prior could form the basis of a \emph{next-best-view} algorithm.
%We believe that a GPU implementation of the voxlet reconstruction part of the algorithm would allow it to run in real time.
%, and for already-made predictions to be replaced as new views of the scene arrive.
%\todo{Also:- 3D object models where they are known

% \section{Acknowledgements}
% Peter Gehler
% Malcolm
% Neill
% Prism group
% Tom
% Peter
% Yotam
% Open source community - python scipy pcl etc

{\small
\bibliographystyle{ieee}
\bibliography{bibtex/strings.bib,bibtex/main.bib,bibtex/crossrefs.bib}
}

%\printbibliography


\end{document}