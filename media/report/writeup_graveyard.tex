\documentclass[10pt,twocolumn,letterpaper]{article}
%\usepackage[latin1]{inputenc}

%\usepackage{url}
%\usepackage{booktabs}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subfigure}
\usepackage{nonfloat}
\usepackage{url}
\usepackage{textcomp} % for textonehalf
%\usepackage{subref}
\graphicspath{{imgs/}}

\usepackage{xspace}
\renewcommand*{\eg}{e.g.\@\xspace}
\renewcommand*{\ie}{i.e.\@\xspace}
\newcommand*{\ea}{et al.\@\xspace}
%\renewcommand{\arraystretch}{1.5}

\cvprfinalcopy % *** Uncomment this line for the final submission


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\definecolor{red}{rgb}{0.95,0.4,0.4}
\definecolor{blue}{rgb}{0.4,0.4,0.95}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{grey}{rgb}{0.6,0.6,0.6}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\note}[1]{\textcolor{blue}{NOTE: #1}}
\newcommand{\status}[1]{\textcolor{blue}{Status: #1}}
\newcommand{\add}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\remove}[1]{\textcolor{grey}{#1}}


\title{Introduction and related work graveyard}

\begin{document}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For a robot to navigate through a cluttered room, an idea of which areas of 3D space are vacant is essential to avoid obstacles.
In order to acquire such a 3D occupancy model, however, a scene must be viewed from multiple angles.
This poses a chicken-and-egg problem: To capture the occupancy model the robot must navigate through the room, while the robot requires an occupancy model to navigate.

To solve this problem an initial estimate of the occupancy of each part of the scene can be made given an initial limited data set; as further observances are made this prior estimate can be overwritten by observed data.
This problem of estimating the unseen areas of the scene is ill-posed and can be very challenging; this is the problem we tackle in this paper.
%, it is possible to make sensible predictions about some areas.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application areas}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Other application areas}
It can be very useful for a computer system to know the geometry of hidden areas of a scene, for reasons such as:
\begin{itemize}
\item A wheeled robot entering a doorway of a cluttered room it has never seen before may wish to navigate to a target object at the far side. However, the floor surface may be occluded for much of the route, by furniture and other clutter.
\item If a single photo is captured from a mobile device with a depth camera attached, it could be useful to be able to relight the scene by digitally adding light sources. To cast realistic shadows, however, the full 3D geometry is required.
\item In multi-view reconstruction problems it can be good to have a prior over the shape of the scene from the first few frames, which can be replaced as more footage comes in. The prior can be used to solve the next-best-view problem, deciding where the camera should be moved to capture the most informative next image of the scene.
\end{itemize}

In these cases, we might wish to make an estimate of the geometry of the world, even if we have not seen the data to confirm the real 3D shape (figure \ref{fig:intro}).

%\begin{itemize}
%\item \textbf{Robotics} --- Helping a robot to plan a path around objects given only a single view of a scene, e.g. from a doorway. Also to help a robot plan grasping position on objects it can only see partial views of.
%\item \textbf{Scene relighting} --- Enabling realistic-looking shadows to be cast from a light moved to a new position in the image.
%\item \textbf{Object repositioning} --- Interactive repositioning of objects in the scene. Knowing the voxel occupancy allows a constraint to be placed on where objects can be moved to. (\eg Zheng \ea, Interactive images)
%\item \textbf{Multi-view reconstruction} --- An intial estimate for voxel occuapncy can be used as a prior during online multi-view reconstruction algorithms. This prior prediction would be overwritten with observed data as more footage is captured. The prior can help to solve the next-best-view problem.



\paragraph{Scene relighting}
\cite{ikeda-acpr-2013} perform scene relighting from a single RGBD image, but cannot recover shadows cast by objects in the scene.
On the other hand, \cite{xiao-cvpr-2014} use the RGBD information to automatically \textit{remove} hard and soft shadows from a single image.

\paragraph{Robotics and path planning}
Voxels or 2D occupancy grids are often used in robotics for path planning, and for maintaining a map of the environment. 
These are typically deterministic approaches, \eg \cite{jetchev-icra-2010}. Sometimes a state model is used to model the states of voxels being occupied, unoccupied or unknown --- the voxel states are then updated as more information is gathered from sensors \cite{toussaint-techreport-2007}.

\cite{stentz-icra-1994} introduced D*, an algorithm similar to A* but for path planing in partially unknown environments. 
The world is in boxes, with arrows pointing to the direction which should be taken.
More recently, \cite{plagemann-iros-2008} used Gaussian processes to fill in missing data in terrain models of environments, effectively treating terrain mapping as a regression problem.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{6DOF object pose estimation}

This relies on having a detailed model of every item that could possibly be seen, which is infeasible for most practical situations.
Similarly, if the class can be inferred then a more generic class-level model can be fitted to a part of the scene \cite{cocias-cgvcv-2013, prisacariu-iccv-2011}; however, this relies on availability of such a class-level model, and the accuracy of an object classifier.


These work in many different ways; \cite{drost-3dimpvt-2012} aggregate pairwise features in a hash table to find the pose estimation --- they assume just one database model, which is assumed to be in the scene. \cite{rusu-iros-2010} first segment the scene, before looking up each segment in their training dataset.
All the methods nonetheless share the training step of rendering a 3D object from multiple angles.

All these methods so far assume that the item in the scene has the exact same geometry of the training instances.
\cite{cocias-cgvcv-2013} take an alternative approach, where they have a single primitive object for each class, which they fit to the scene and allow to deform to account for inter-class variation.
\cite{prisacariu-iccv-2011} present a more developed version of this idea, where the latent axes of variation of the shape of the object being fitted are allowed to deform during the pose estimation. 
These latent axes are found using Gaussian processes.
This adaptive fitting of generic primitives to the specific instances seen in an image has seen practical application in the graphics community, where it has been used for an impressive interactive photo-editing application \cite{kholgade-siggraph-2014}.

\cite{zhang-eccv-2014} do indoor 3D semantic bounding box prediction using a single 2D panoramic image. They make heavy use of context to drive their predictions.
In our work, by not restricting ourselves to trying to find semantic correspondences, we are able to model the shape of a far greater range of test scenes without making any assumptions about their semantic makeup.

Render CAD models from multiple directions, combine with sliding window detector: \cite{song-eccv-2014}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Symmetry}
Law and Aliaga \cite{law-cviu-2010} use symmetry to complete partial views. 
They are limited to only completing partial views; they rely on all objects being symmetrical; and they need user input.
Similarly, \cite{thrun-iccv-2005} and \cite{kroemer-humanoids-2012} use symmetry to complete models from a single depth view. 
They both demonstrate results on isolated objects, but not on more complex or cluttered scenes. 
The constraint of requiring axes of symmetry to both be present and accurately detected is a large limiting factor with these approaches.
also: \url{http://publik.tuwien.ac.at/files/PubDat_198335.pdf}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Indoor scenes}
\cite{bao-wacv-2014} use multiple images, before SfM and segmentation of the images. 
They then generate hypotheses of the layout. 
The hypotheses are chosen by evaluating their geometric cost, which is manually defined. 

Satkin \ea \cite{satkin-bmvc-2012} use 3D models to do scene understanding from a single image, and they demonstrate results on room scenes such as bedrooms and living rooms. 
A key interest for me is that they first propose hypotheses, before rendering them with OpenGL.
This then feeds into a hypothesis scorer which says if their ideas are any good. 
They form their proposals in an expensive way: they moving the cad model along the (x, y) scene axes, and render the model in every location, and in each of 4 rotation configurations!

Similarly, \cite{zhang-iccv-2013} infer the clutter and layout and support and segmentation and labelling from an RGBD image (using the NYU dataset).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Filling holes in meshes}
In the graphics community, there is a lot of work looking at the completion of missing and occluded parts of 3D meshes. 
For example, \cite{podolak-esgp-2005} fill holes in a mesh by enforcing watertightness across an octree structure, while \cite{schnabel-eurographics-2009} complete meshes by using primitives extracted from the areas of the scan without missing detail. 
A good overview of such mesh completion algorithms are given in \cite{ju-cst-2009}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Super-resolution}
A seminal paper in this field is \cite{freeman-ijcv-2000}, patches from generic training images are used in super-resolution for coarse-grained images. They use Bayesian belief propagation to find the maximum posterior probability over a Markov network defined over the image.
\cite{chang-tor-2007} complete a robot's map of an environment using parts of the already observed world to  `patch in' missing areas of the current map.
Depth images are a more complex beast than meshes and 2D images, and RGBD datasets on the order of millions of images do not yet exist, so we need to take a different approach.


\paragraph{General motivation??}
\cite{nan-acm-2012}.


\paragraph{Further related work:}
\begin{itemize}
\item Teaching 3D Geometry to Deformable Part Models
\item Things which cited voxel CRF...
\item Also: Amodal volume completion: 3D visual completion
\item Class specific: Single and sparse view 3D reconstruction by learning shape priors
\item Big issue with ground truth. Most papers don't have proper ground truth and therefore can only really do qualitative results, \eg \cite{all the papers...}.
\item Additional bounding box technique I've left out of main paper:
\cite{choi-cvpr-2013}
\end{itemize}

{\small
%\bibliographystyle{plain}
\bibliographystyle{ieee}
\bibliography{bibtex/strings.bib,bibtex/main.bib,bibtex/crossrefs.bib}
}


\end{document}